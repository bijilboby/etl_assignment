# ğŸ§  Mini Data Warehouse using CSVs, Python, and DuckDB

This project builds a **mini data warehouse** using raw CSV files, Python for ETL, and DuckDB for SQL analytics â€” without using a traditional database.

---

## ğŸ“‚ Raw Data Description

All raw files are placed inside `raw_data/` and contain intentional **data quality issues** like:

| File                | Anomalies Included                                        |
|---------------------|-----------------------------------------------------------|
| `customers_raw.csv` | âœ… Nulls in email/name, duplicate IDs and rows           |
| `products_raw.csv`  | âœ… Inconsistent casing (`"   electronics "`), null prices |
| `orders_raw.csv`    | âœ… Null quantity/order_date, duplicate order IDs          |

These messy CSVs simulate real-world raw data that must be cleaned and modeled.

---

## âš™ï¸ ETL Process with Python (`etl_pipeline.py`)

The file `etl_pipeline.py` handles the complete pipeline:

### ğŸ§¼ Features:
- Removes **duplicate** and **null** rows
- Cleans text:
  - Lowercases `category`
  - Strips and lowercases `email`
- Standardizes `order_date` to `YYYY-MM-DD`
- Creates a **date dimension** from order dates
- Calculates `total_amount = quantity * price`
- Outputs 4 cleaned CSV tables forming a **star schema**

### â–¶ï¸ To Run:

```bash
python etl_pipeline.py
```

---

## â­ Star Schema Design

The warehouse follows a classic **Star Schema**:

### ğŸ”¸ Central Fact Table

- **`fact_orders`**: contains `order_id`, `product_id`, `customer_id`, `date_id`, `quantity`, `total_amount`

### ğŸ”¹ Dimension Tables

- **`dim_customers`**: information on customers  
- **`dim_products`**: cleaned product information  
- **`dim_date`**: derived from `order_date` with columns like `year`, `month`, `day`, `weekday`

### ğŸ“Š Star Schema Diagram

![Star Schema](./star_schema.svg)

*Visual representation of the data warehouse star schema with fact and dimension tables*

---

## ğŸ§  SQL Analytics with DuckDB

All SQL queries are modularized into individual `.sql` files inside the `sql/` folder for better organization and maintainability.

### ğŸ” Current SQL Query Files

| Query File                           | Description                              |
|--------------------------------------|------------------------------------------|
| `Top_5_spends_by_customers.sql`      | Top 5 customers by total spend          |
| `Monthly_revenue_trend.sql`          | Revenue trend per month/year            |
| `Best_selling_product_categorie.sql`| Best-selling product category        |
| `Top_cities_by_revenue.sql`          | City-wise revenue breakdown             |

---

## ğŸ“„ SQL Query Execution: `duckdb_analysis.py`

This script is responsible for executing the SQL queries using [DuckDB](https://duckdb.org/), an in-process SQL OLAP database.

### ğŸ›  What it does:
- Loads the **transformed warehouse CSVs** into **in-memory DuckDB tables**
- **Dynamically reads all `.sql` files** from the `sql/` directory
- Executes each query automatically and displays formatted results
- Handles errors gracefully for individual queries

### ğŸ”§ Key Features:
- **Modular SQL queries**: Each analysis is in its own SQL file
- **Dynamic execution**: Automatically finds and runs all SQL files
- **Error handling**: Continues execution even if one query fails
- **Formatted output**: Clean display with query names and results

### â–¶ï¸ To Run:

Make sure you have `duckdb` installed:

```bash
pip install duckdb
```

Then execute the analysis:

```bash
python duckdb_analysis.py
```

---

## ğŸ—ï¸ Project Structure

```
ETL Assignment/
â”œâ”€â”€ ğŸ“ raw_data/
â”‚   â”œâ”€â”€ ğŸ“„ customers_raw.csv
â”‚   â”œâ”€â”€ ğŸ“„ products_raw.csv
â”‚   â””â”€â”€ ğŸ“„ orders_raw.csv
â”œâ”€â”€ ğŸ“ transformed_data/
â”‚   â”œâ”€â”€ ğŸ“„ dim_customers.csv
â”‚   â”œâ”€â”€ ğŸ“„ dim_products.csv
â”‚   â”œâ”€â”€ ğŸ“„ dim_date.csv
â”‚   â””â”€â”€ ğŸ“„ fact_orders.csv
â”œâ”€â”€ ğŸ“ sql/
â”‚   â”œâ”€â”€ ğŸ“„ Top_5_spends_by_customers.sql
â”‚   â”œâ”€â”€ ğŸ“„ Monthly_revenue_trend.sql
â”‚   â”œâ”€â”€ ğŸ“„ Best_selling_product_category.sql
â”‚   â””â”€â”€ ğŸ“„ Top_cities_by_revenue.sql
â”œâ”€â”€ ğŸ etl_pipeline.py
â”œâ”€â”€ ğŸ¦† duckdb_analysis.py
â”œâ”€â”€ ğŸ–¼ï¸ star_schema.svg
â””â”€â”€ ğŸ“‹ README.md
```

---

## ğŸ“Š Sample Analysis Results

### ğŸ† Top 5 Customers by Spend
```
              name  total_spend
0      Joshua Ryan      1529.95
1  Nicholas Murphy      1208.97
2    Steven Foster       951.39
3    Melissa Price       859.97
4    Andrew George       699.98
```

### ğŸ“ˆ Best Selling Product Category
```
      category  total_sold
0  electronics        32.0

```

### ğŸ™ï¸ Top Cities by Revenue
```
              city  city_revenue
0    South Charles       1529.95
1    Lake Kimberly       1208.97
2      East Rachel        951.39
3      Port Joseph        859.97
4    South William        699.98
```

### ğŸ“Š Monthly Revenue Trend
```
    year  month  monthly_revenue
0   2023     11           873.87
1   2023     12          1678.42
2   2024      4          1399.96
3   2024      7          1163.90
4   2024      6           753.37
```

---

## ğŸš€ Getting Started

1. **Clone the repository**
2. **Install dependencies:**
   ```bash
   pip install pandas duckdb
   ```
3. **Run the ETL pipeline:**
   ```bash
   python etl_pipeline.py
   ```
4. **Execute analytics:**
   ```bash
   python duckdb_analysis.py
   ```

---

## ğŸ› ï¸ Technologies Used

- **Python** - ETL processing and data transformation
- **Pandas** - Data manipulation and cleaning
- **DuckDB** - In-memory SQL analytics engine
- **CSV** - Data storage format

---

*Built as part of the MITZ Internship ETL Assignment*